{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "2.01 - Binary classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olivermueller/aml4ta-2021/blob/main/Session_02/2_01_Binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrKCbjZsqpo"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pblMcujrVt7-"
      },
      "source": [
        "# Set up Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NpTS4Z2Vvzy"
      },
      "source": [
        "# Install packages\n",
        "!pip install pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-DU0hkyVyPi"
      },
      "source": [
        "# <font color=\"#003660\">Week 2: Predicting with Bags of Words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhy42GjRV3ON"
      },
      "source": [
        "# <font color=\"#003660\">Notebook 1: Binary Classification</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
        "        ... transform raw text into a term-document matrix, <br>\n",
        "        ... train a binary classifier on the term-document matrix, and <br> ... and compete in a Kaggle competition.\n",
        "    </font>\n",
        "</div>\n",
        "</center>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6vVpwIFsqps"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "As always, we first need to load a number of required Python packages:\n",
        "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
        "- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n",
        "- `getpass` provides function to safely enter passwords.\n",
        "- `spacy` offers industrial-strength natural language processing.\n",
        "- `sklearn` is the de-facto standard machine learning package in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMrhkr83sqpt"
      },
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import getpass\n",
        "import spacy\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZd82t53sqpu"
      },
      "source": [
        "# Load documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrCafxksqpv"
      },
      "source": [
        "We load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda to get them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSIGfKcXsqpv"
      },
      "source": [
        "# Get credentials\n",
        "user = input(\"Username: \")\n",
        "passwd = getpass.getpass(\"Password: \")\n",
        "server = input(\"Server: \")\n",
        "db = input(\"Database: \")\n",
        "\n",
        "# Create an engine instance (SQLAlchemy)\n",
        "engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n",
        "\n",
        "# Define SQL query\n",
        "sql_query = \"SELECT * FROM WineDataset\"\n",
        "\n",
        "# Query dataset (pandas)\n",
        "corpus = pd.read_sql(sql=sql_query, con=engine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMzZ9re_yY5K"
      },
      "source": [
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NsIQrLiSEak"
      },
      "source": [
        "corpus.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v8oiPAcsqpx"
      },
      "source": [
        "# Preprocess documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7psnR5cmQLBx"
      },
      "source": [
        "Split data into training, validation, and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSObnTaWdgdM"
      },
      "source": [
        "training = corpus[corpus[\"testset\"] == 0]\n",
        "validation = training.iloc[80000:100000,]\n",
        "training = training.iloc[0:80000,]\n",
        "test = corpus[corpus[\"testset\"] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m05pjMr8Rvxs"
      },
      "source": [
        "print(training.shape)\n",
        "print(validation.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSrHVdoZsqpy"
      },
      "source": [
        "Perform standard NLP preprocessing steps on the training set using spaCy. To speed up things, we disable some components of spaCy's standard NLP pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh4KVmP6sqpy"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
        "\n",
        "def spacy_prep(dataset):\n",
        "  dataset = dataset.to_dict(\"records\")\n",
        "  for i, entry in enumerate(dataset):\n",
        "      text = nlp(entry[u'description'])\n",
        "      tokens_to_keep = []\n",
        "      for token in text:\n",
        "          if token.is_alpha and not token.is_stop:\n",
        "              tokens_to_keep.append(token.lemma_.lower())\n",
        "      entry[u'description_prep'] = \" \".join(tokens_to_keep)\n",
        "  dataset = pd.DataFrame(dataset)\n",
        "  return(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4rKCZRs9tlj"
      },
      "source": [
        "training = spacy_prep(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tothp_Ssqpz"
      },
      "source": [
        "Display the first couple of lines of the preprocessed descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyLoiearsqpz"
      },
      "source": [
        "training[\"description_prep\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYQppmeBQUtX"
      },
      "source": [
        "# Vectorize documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAXDnM1Hsqp1"
      },
      "source": [
        "Vectorization is the process of turning a collection of text documents into numerical feature vectors. \n",
        "\n",
        "We will use the **Bag of Words (BoW)** model for vectorization. In the BoW model, a corpus of documents is represented by a matrix with one row per document and one column per word occurring in the corpus. The cell values will either be simple frequency counts (How often does a word appear in a document?), or the term frequency (tf) times the inverse document frequency (idf) of a term. The idea of tf-idf is to scale down the impact of words that occur very frequently in a given corpus and that are therefore less informative than features that occur only in a small fraction of the corpus. Note that the BoW model completely ignores information about the position and sequences of the words in the document.\n",
        "\n",
        "In `sklearn`, the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) creates a term-document matrix with (normalized) term frequencies and the [`TfIdfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) creates a term-document matrix with tf-idf weighting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbeTq9S9sqp1"
      },
      "source": [
        "count_vect = CountVectorizer(min_df=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06DEGrExsqp1"
      },
      "source": [
        "Apply the CountVectorizer object to the review texts of the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AemriJOAsqp1"
      },
      "source": [
        "X_training = count_vect.fit_transform(training[\"description_prep\"].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OV17A_2sqp2"
      },
      "source": [
        "Display an extract of the generated term-document matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLq7fYBy3J2q"
      },
      "source": [
        "X_training.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQd_dVlsqp2"
      },
      "source": [
        "X_training[0:20,0:20].todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTlJ4BDsqp3"
      },
      "source": [
        "Store the labels that we want to predict in a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUpaU5Pgsqp3"
      },
      "source": [
        "y_training = training[\"verygood\"]\n",
        "y_training.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biwWaNjNsqp4"
      },
      "source": [
        "# Train classifier on training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTHaUsAosqp4"
      },
      "source": [
        "Fit a logistic regression classification with the term-document matrix as the features and the wine quality (i.e., `verygood` variable) as the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZkAh7oesqp4"
      },
      "source": [
        "clf = LogisticRegression().fit(X_training, y_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuAVWgC8sqp4"
      },
      "source": [
        "Test whether classifier is working by predicting the quality of a short fake review. We apply the same NLP preprocessing steps and reuse the `count_vect` object to generate features in the same way as we did for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrjU4Uj4-jpx"
      },
      "source": [
        "doc_new = {'index': [1], \n",
        "           'description': ['This is a good wine']}\n",
        "\n",
        "doc_new_df = pd.DataFrame.from_dict(doc_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B16iOiWsqp5"
      },
      "source": [
        "doc_new_df_prep = spacy_prep(doc_new_df)\n",
        "doc_new_df_prep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sD7SeyD4nig"
      },
      "source": [
        "X_new = count_vect.transform(doc_new_df_prep[\"description_prep\"])\n",
        "predicted = clf.predict(X_new)\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6zZzlq4sqp5"
      },
      "source": [
        "Instead of predicting binary labels, we can also predict probabilities of the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks6yRPyEsqp5"
      },
      "source": [
        "predicted_prob = clf.predict_proba(X_new)\n",
        "print(clf.classes_)\n",
        "print(predicted_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "663LU5XQsqp5"
      },
      "source": [
        "# Evaluate accuracy on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8-z8rqVsqp6"
      },
      "source": [
        "Before trying to predict the labels for the official test set, we evaluate the predictive accurcay of our model on the validation set. Again, we apply the same NLP preprocessing steps, reuse the `count_vect` object, and store `X` and `y` in separate data structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCZDzJOR51SZ"
      },
      "source": [
        "validation = spacy_prep(validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiUIhoK0sqp6"
      },
      "source": [
        "X_validation = count_vect.transform(validation[\"description_prep\"])\n",
        "y_validation = validation[\"verygood\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4upAshmAsqp6"
      },
      "source": [
        "Call the predict function of our model with the validation data and calculate precision, recall and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ngkprkgsqp6"
      },
      "source": [
        "predictions_validation = clf.predict(X_validation)\n",
        "print(metrics.classification_report(y_validation, predictions_validation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOCv-dHNsqp6"
      },
      "source": [
        "# Interpret model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB06C4vFsqp7"
      },
      "source": [
        "Logistic regression is typically not the most accurate classification model, but one big advantage is that it can be interpreted by looking at the coefficients of the input features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOHsVlNGsqp7"
      },
      "source": [
        "coeffs = clf.coef_[0].tolist()\n",
        "words = count_vect.get_feature_names()\n",
        "words_with_coeffs = pd.DataFrame(coeffs, words, columns=[\"coeff\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaHsYC1xsqp7"
      },
      "source": [
        "These are the words with the most *negative* impact.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc6X4kaFsqp7"
      },
      "source": [
        "words_with_coeffs.sort_values(\"coeff\", ascending=True).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo2Zz0qrsqp8"
      },
      "source": [
        "And these are the words with the most *positive* impact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBYjvIJnsqp8"
      },
      "source": [
        "words_with_coeffs.sort_values(\"coeff\", ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-kUHvqUT7H"
      },
      "source": [
        "# Make predictions on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOAiyw6_5ZJn"
      },
      "source": [
        "Preprocess and vectorize the review texts of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljCV8CjF5m-B"
      },
      "source": [
        "test = spacy_prep(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKxkVjlsUXCQ"
      },
      "source": [
        "X_test = count_vect.transform(test[\"description_prep\"])\n",
        "predictions_test = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnUWSu8N5oZN"
      },
      "source": [
        "Create a dataframe with the indices and predictions and save it as a CSV file (which we can upload to Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTBW2SnMUkjm"
      },
      "source": [
        "my_submission = pd.DataFrame({'index': test[\"index\"],\n",
        "                              'verygood':predictions_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwZ8MIcVAjl"
      },
      "source": [
        "my_submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzQ2NzHyVvGB"
      },
      "source": [
        "my_submission.to_csv(\"my_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NHp_iYqX2Ob"
      },
      "source": [
        "# Define a pipeline and tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW59HxdX60bd"
      },
      "source": [
        "Typically, we want to try out different preprocessing strategies and/or different classification algorithms. The concept of a **pipeline** in `sklearn` is very usuful to streamline this process.\n",
        "\n",
        "The purpose of a pipeline is to bundle several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a `__`, as in the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGl_loPlZjJz"
      },
      "source": [
        "clf_pipe = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', LogisticRegression()),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydgFnF9ZjFc"
      },
      "source": [
        "parameters = {\n",
        "    'vect__min_df': (2, 10, 100)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlvsvhdp7Ty0"
      },
      "source": [
        "With the pipeline and its parameters, it is possible to run an exhaustive search of the best parameters on a grid of possible values and evaluate their effects on the predictive accuracy using k-fold cross validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM1CqR7wZrEb"
      },
      "source": [
        "clf_pipe_gs = GridSearchCV(clf_pipe, parameters, cv=5, scoring=\"f1_macro\", n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-iyvUPMZuw_"
      },
      "source": [
        "clf_pipe_gs = clf_pipe_gs.fit(training[\"description_prep\"], training[\"verygood\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-wgKe0FabXj"
      },
      "source": [
        "pd.DataFrame(clf_pipe_gs.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaeEi9Wd773N"
      },
      "source": [
        "After the grid search has been performed and the best parameter values have been determined, we can use the fitted pipeline object just like a normal model (e.g., call the predict method with new data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcdAFAMgaFot"
      },
      "source": [
        "predictions_validation = clf_pipe_gs.predict(validation[\"description_prep\"])\n",
        "print(metrics.classification_report(validation[\"verygood\"], predictions_validation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu2VTatR9DuV"
      },
      "source": [
        "predictions_test = clf_pipe_gs.predict(test[\"description_prep\"])\n",
        "my_submission = pd.DataFrame({'index': test[\"index\"],\n",
        "                              'verygood':predictions_test})\n",
        "my_submission.to_csv(\"my_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFi7MHRIX5v7"
      },
      "source": [
        "For more tips and tricks on parameter tuning using grid search for text data, see: [https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
      ]
    }
  ]
}