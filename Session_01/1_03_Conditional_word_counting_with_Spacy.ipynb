{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "1.03 - Conditional word counting with Spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olivermueller/aml4ta-2021/blob/main/Session_01/1_03_Conditional_word_counting_with_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqqfUmU7Rsk_"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCDEMc3I3-y"
      },
      "source": [
        "# Set up Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtb_6VbuwUOU"
      },
      "source": [
        "# Install missing packages\n",
        "!pip install pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTr0oOHpRuwL"
      },
      "source": [
        "# <font color=\"#003660\">Week 1: Basics of Natural Language Processing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21pvAXnqXAF"
      },
      "source": [
        "# <font color=\"#003660\">Notebook 3: Conditional Word Counting with Spacy</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
        "        ... perform NLP preprocessing with Spacy.</font>\n",
        "</div>\n",
        "</center>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtu_FbD9qXAJ"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "As always, we first need to load a number of required Python packages:\n",
        "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
        "- `NLTK` is a leading platform for building Python programs to work with human language data.\n",
        "- `Spacy` is a library for for industrial-strength natural language processing.\n",
        "- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n",
        "- `getpass` provides function to safely enter passwords.\n",
        "- `altair` is a visualization library based on the grammar of graphics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BAE0ESOwqXAK"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from sqlalchemy import create_engine\n",
        "import getpass\n",
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUMZ6WKIV6Zc"
      },
      "source": [
        "# Test drive Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVRpEHMh6S1y"
      },
      "source": [
        "spaCy is an open-source library for Natural Language Processing (NLP) in Python. It helps you build NLP applications that process and understand large volumes of unstructured text. One of the main features of spaCy are linguistic annotations that give you insights into a text’s grammatical structure (e.g., word order, types of words, parts of speech, grammatical roles and relations).\n",
        "\n",
        "At the center of spaCy is the processing pipeline, an object which is usually called `nlp`. The pipeline is build on top of a language-specific machine learning model and a set of handcrafted rules.\n",
        "\n",
        "The nlp pipeline contains different components, each specialized for a specific NLP task. \n",
        "\n",
        "[More...](https://spacy.io/usage/spacy-101#whats-spacy)\n",
        "\n",
        "<center><br><img src=\"\n",
        "https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\"/><br></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8JP2C08tOO"
      },
      "source": [
        "The following code creates a processing pipeline based on the `en_core_web_sm` model and assigns it to the variable `nlp`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcBsfj0ZVl_F"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq50IrBq84rg"
      },
      "source": [
        "Let's feed the `nlp` object with a simple sentence. When you process a text with the `nlp` object, spaCy creates a `doc` object. The `doc` lets you access information about the text in a structured way. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb6ohjGMV-Pu"
      },
      "source": [
        "doc = nlp(u\"Yesterday, I went to five pubs in Oxford.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wJ3wY5oXJdy"
      },
      "source": [
        "The first component of every spaCy pipeline is the `tokenizer`, which segments an unstructured text into words, punctuation, and so on. These `tokens` are the main contents of the `doc` object. [More...](https://spacy.io/usage/spacy-101#annotations-token)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhHHoEUaWPpG"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdP8BASnXLF1"
      },
      "source": [
        "The tokens contain many useful attributes. For example, we can access the `lemma` of each token. [More...](https://spacy.io/usage/linguistic-features#lemmatization\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk9gJUiqWTpt"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text, \"->\", token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShidHKjEXMzt"
      },
      "source": [
        "`Part-of-speech` tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. Examples of POS include words as nouns, verbs, adjectives, adverbs, etc. [More](https://spacy.io/usage/linguistic-features#pos-tagging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J2uHwQHXeV3"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text, \"->\", token.pos_, \"(\", spacy.explain(token.pos_), \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq5wwmmFXilm"
      },
      "source": [
        "We can also recognize so-called named entities. A `named entity` is a “real-world object” that’s assigned a name – for example, a person, a country, or  a product. spaCy can recognize various types of named entities in a document. [More...](https://spacy.io/usage/linguistic-features#named-entities)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc2yurJiXlw_"
      },
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, \"->\", ent.label_, \"(\", spacy.explain(ent.label_), \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PhOPcrmqXAM"
      },
      "source": [
        "# Load documents\n",
        "This time, we load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda to get them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aV5f4froqXAM"
      },
      "source": [
        "# Get credentials\n",
        "user = input(\"Username: \")\n",
        "passwd = getpass.getpass(\"Password: \")\n",
        "server = input(\"Server: \")\n",
        "db = input(\"Database: \")\n",
        "\n",
        "# Create an engine instance (SQLAlchemy)\n",
        "engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n",
        "\n",
        "# Define SQL query\n",
        "sql_query = \"SELECT * FROM BillboardLyrics\"\n",
        "\n",
        "# Query dataset (pandas)\n",
        "corpus = pd.read_sql(sql=sql_query, con=engine)\n",
        "\n",
        "# Sample\n",
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1cHok_qXAM"
      },
      "source": [
        "corpus.iloc[42]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPv6koXpqXAQ"
      },
      "source": [
        "# Preprocess documents with Spacy\n",
        "Tokenization, stopword removal and lemmatization in one go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqTCwDpVY22T"
      },
      "source": [
        "docs_prep = corpus.to_dict(\"records\")\n",
        "for i, entry in enumerate(docs_prep):\n",
        "  if entry[\"Lyrics\"]:\n",
        "    doc = nlp(entry[\"Lyrics\"])\n",
        "    tokens_prep = [] \n",
        "    for token in doc:\n",
        "      if token.is_alpha and not token.is_stop:\n",
        "        tokens_prep.append(token.lemma_)\n",
        "    entry[\"Lyrics_prep\"] = tokens_prep\n",
        "  else:\n",
        "    entry[\"Lyrics_prep\"] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QIP7RiZoLS"
      },
      "source": [
        "docs_prep[42]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXXYgWwBqXAT"
      },
      "source": [
        "# Conditional word counting\n",
        "We seperately count words for each condition, that is, for each year. Unfortunately, this time we have to do this \"by hand\" and iterate through all docs and tokens and increase the token count for the respective condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PPKYDzHDqXAU"
      },
      "source": [
        "cfreq = nltk.ConditionalFreqDist()\n",
        "\n",
        "for doc in docs_prep:\n",
        "  for token in doc[\"Lyrics_prep\"]:\n",
        "    condition = doc[\"Year\"]\n",
        "    cfreq[condition][token] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DB3itP_7dze"
      },
      "source": [
        "cfreq[2010]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34QLgxiTKlVN"
      },
      "source": [
        "# Time series of word occurences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi0AhH7mqXAV"
      },
      "source": [
        "For all years between 1965 and 2015, get the frequency of the word \"money\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PKo19RFMqXAX"
      },
      "source": [
        "word = u\"money\"\n",
        "years = range(1965,2016)\n",
        "occurences = []\n",
        "for year in years:\n",
        "  occurences.append(cfreq[year][word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZYCBOauK4oK"
      },
      "source": [
        "Merge the years and the word occurcences in one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG4vql4MK3rM"
      },
      "source": [
        "timeseries = pd.DataFrame(list(zip(years, occurences)),\n",
        "              columns=['years','count'])\n",
        "timeseries['years'] = pd.to_datetime(timeseries['years'], format='%Y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGGmV3kqXAX"
      },
      "source": [
        "Plot the time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPP0bsYaqXAY"
      },
      "source": [
        "alt.Chart(timeseries).mark_line().encode(\n",
        "    x='years',\n",
        "    y='count'\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoHxD3Sf3WTw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}