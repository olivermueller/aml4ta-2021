{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "colab": {
      "name": "1.02 - Conditional word counting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olivermueller/amlta2021/blob/main/Session_01/1_02_Conditional_word_counting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqqfUmU7Rsk_"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqCQchAp92Ns"
      },
      "source": [
        "XXX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCDEMc3I3-y"
      },
      "source": [
        "# Set up Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Colab Notebooks/AMLTA2021/Session_01\n",
        "\n",
        "!pip install pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTr0oOHpRuwL"
      },
      "source": [
        "# <font color=\"#003660\">Week 1: Basics of Natural Language Processing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21pvAXnqXAF"
      },
      "source": [
        "# <font color=\"#003660\">Notebook 2: Conditional Word Counting</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
        "        ... bla bla bla, and<br>\n",
        "        ... bla bla bla.</font>\n",
        "</div>\n",
        "</center>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtu_FbD9qXAJ"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "As always, we first need to load a number of required Python packages:\n",
        "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
        "- `NLTK` is a leading platform for building Python programs to work with human language data.\n",
        "- `altair` is a visualization library based on the grammar of graphics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BAE0ESOwqXAK"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZVcs4V9JOhJ"
      },
      "source": [
        "To work with the `NLTK` package, you also need to download some additional data (e.g., stopword lists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bljTpl4JPtx",
        "outputId": "a2b8a022-0e14-4413-fef2-6cf0ad11c378"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PhOPcrmqXAM"
      },
      "source": [
        "# Load documents\n",
        "This time, we want to analyze documents with regards to some metadata (i.e., year of publication). Each document is stored in a dictionary with two keys (`text` and `year`). The corpus is stored as a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aV5f4froqXAM"
      },
      "source": [
        "corpus = [\n",
        "    {\"text\":\"Hello World\", \"year\":2015},\n",
        "    {\"text\":\"How are you today?\", \"year\":2015},\n",
        "    {\"text\":\"The world is nice\", \"year\":2016},\n",
        "    {\"text\":\"The weather is also nice\", \"year\":2016},\n",
        "    {\"text\":\"Yesterday, the weather was also nice\", \"year\":2017},\n",
        "    {\"text\":\"I own two bicycles\", \"year\":2017},\n",
        "    {\"text\":\"I love to ride my bicycle\", \"year\":2018}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1cHok_qXAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615caa50-2bad-4f95-8b9c-9035ee1025a5"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Hello World', 'year': 2015},\n",
              " {'text': 'How are you today?', 'year': 2015},\n",
              " {'text': 'The world is nice', 'year': 2016},\n",
              " {'text': 'The weather is also nice', 'year': 2016},\n",
              " {'text': 'Yesterday, the weather was also nice', 'year': 2017},\n",
              " {'text': 'I own two bicycles', 'year': 2017},\n",
              " {'text': 'I love to ride my bicycle', 'year': 2018}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPv6koXpqXAQ"
      },
      "source": [
        "# Preprocess documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MpKatMPqXAQ"
      },
      "source": [
        "We make a copy of the corpus dictionary and iterate over its entries to add a `tokens` field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZISE6qkJqXAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ececda-47d3-4994-b667-741b770456d7"
      },
      "source": [
        "docs_tokenized = corpus[:]\n",
        "for i, entry in enumerate(docs_tokenized):\n",
        "    entry[\"tokens\"] = nltk.word_tokenize(entry[\"text\"])\n",
        "docs_tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Hello World', 'tokens': ['Hello', 'World'], 'year': 2015},\n",
              " {'text': 'How are you today?',\n",
              "  'tokens': ['How', 'are', 'you', 'today', '?'],\n",
              "  'year': 2015},\n",
              " {'text': 'The world is nice',\n",
              "  'tokens': ['The', 'world', 'is', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'The weather is also nice',\n",
              "  'tokens': ['The', 'weather', 'is', 'also', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'Yesterday, the weather was also nice',\n",
              "  'tokens': ['Yesterday', ',', 'the', 'weather', 'was', 'also', 'nice'],\n",
              "  'year': 2017},\n",
              " {'text': 'I own two bicycles',\n",
              "  'tokens': ['I', 'own', 'two', 'bicycles'],\n",
              "  'year': 2017},\n",
              " {'text': 'I love to ride my bicycle',\n",
              "  'tokens': ['I', 'love', 'to', 'ride', 'my', 'bicycle'],\n",
              "  'year': 2018}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef289TzMqXAR"
      },
      "source": [
        "And we iterate again over the corpus to transform all tokens to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJtJqo0qXAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1683e186-c5d4-4b47-ff4a-8dd538ee10f9"
      },
      "source": [
        "docs_tokenized_lower = docs_tokenized[:]\n",
        "for i,entry in enumerate(docs_tokenized_lower):\n",
        "    tokens_lower = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        tokens_lower.append(token.lower())\n",
        "    entry[\"tokens\"] = tokens_lower\n",
        "docs_tokenized_lower"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Hello World', 'tokens': ['hello', 'world'], 'year': 2015},\n",
              " {'text': 'How are you today?',\n",
              "  'tokens': ['how', 'are', 'you', 'today', '?'],\n",
              "  'year': 2015},\n",
              " {'text': 'The world is nice',\n",
              "  'tokens': ['the', 'world', 'is', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'The weather is also nice',\n",
              "  'tokens': ['the', 'weather', 'is', 'also', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'Yesterday, the weather was also nice',\n",
              "  'tokens': ['yesterday', ',', 'the', 'weather', 'was', 'also', 'nice'],\n",
              "  'year': 2017},\n",
              " {'text': 'I own two bicycles',\n",
              "  'tokens': ['i', 'own', 'two', 'bicycles'],\n",
              "  'year': 2017},\n",
              " {'text': 'I love to ride my bicycle',\n",
              "  'tokens': ['i', 'love', 'to', 'ride', 'my', 'bicycle'],\n",
              "  'year': 2018}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTjfiR6qXAS"
      },
      "source": [
        "And lemmatize all tokens..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NidxTidPqXAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c12d25-4aad-4518-c656-29e0aff75c52"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "docs_tokenized_lower_lemmatized = docs_tokenized_lower[:]\n",
        "for i,entry in enumerate(docs_tokenized_lower_lemmatized):\n",
        "    tokens_lemmatized = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
        "    entry[\"tokens\"] = tokens_lemmatized\n",
        "docs_tokenized_lower_lemmatized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Hello World', 'tokens': ['hello', 'world'], 'year': 2015},\n",
              " {'text': 'How are you today?',\n",
              "  'tokens': ['how', 'are', 'you', 'today', '?'],\n",
              "  'year': 2015},\n",
              " {'text': 'The world is nice',\n",
              "  'tokens': ['the', 'world', 'is', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'The weather is also nice',\n",
              "  'tokens': ['the', 'weather', 'is', 'also', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': 'Yesterday, the weather was also nice',\n",
              "  'tokens': ['yesterday', ',', 'the', 'weather', u'wa', 'also', 'nice'],\n",
              "  'year': 2017},\n",
              " {'text': 'I own two bicycles',\n",
              "  'tokens': ['i', 'own', 'two', u'bicycle'],\n",
              "  'year': 2017},\n",
              " {'text': 'I love to ride my bicycle',\n",
              "  'tokens': ['i', 'love', 'to', 'ride', 'my', 'bicycle'],\n",
              "  'year': 2018}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-CKFTvqXAT"
      },
      "source": [
        "Finally, we iterate one last time over the corpus to remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41c8Sw83qXAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea42f9f8-cee4-427f-8c63-3790b2d44f0c"
      },
      "source": [
        "docs_tokenized_lower_lemmatized_cleaned = docs_tokenized_lower_lemmatized[:]\n",
        "for i,entry in enumerate(docs_tokenized_lower_lemmatized_cleaned):\n",
        "    tokens_cleaned = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        if (token not in stopwords.words('english')):\n",
        "            tokens_cleaned.append(token)\n",
        "    entry[\"text\"] = tokens_cleaned\n",
        "docs_tokenized_lower_lemmatized_cleaned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': ['hello', 'world'], 'tokens': ['hello', 'world'], 'year': 2015},\n",
              " {'text': ['today', '?'],\n",
              "  'tokens': ['how', 'are', 'you', 'today', '?'],\n",
              "  'year': 2015},\n",
              " {'text': ['world', 'nice'],\n",
              "  'tokens': ['the', 'world', 'is', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': ['weather', 'also', 'nice'],\n",
              "  'tokens': ['the', 'weather', 'is', 'also', 'nice'],\n",
              "  'year': 2016},\n",
              " {'text': ['yesterday', ',', 'weather', u'wa', 'also', 'nice'],\n",
              "  'tokens': ['yesterday', ',', 'the', 'weather', u'wa', 'also', 'nice'],\n",
              "  'year': 2017},\n",
              " {'text': ['two', u'bicycle'],\n",
              "  'tokens': ['i', 'own', 'two', u'bicycle'],\n",
              "  'year': 2017},\n",
              " {'text': ['love', 'ride', 'bicycle'],\n",
              "  'tokens': ['i', 'love', 'to', 'ride', 'my', 'bicycle'],\n",
              "  'year': 2018}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXXYgWwBqXAT"
      },
      "source": [
        "# Conditional word counting\n",
        "We seperately count words for each condition, that is, for each year. Unfortunately, this time we have to do this \"by hand\" and iterate through all docs and tokens and increase the token count for the respective condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PPKYDzHDqXAU"
      },
      "source": [
        "cfreq = nltk.ConditionalFreqDist()\n",
        "\n",
        "for doc in docs_tokenized_lower_lemmatized_cleaned:\n",
        "    for token in doc[\"text\"]:\n",
        "        condition = doc[\"year\"]\n",
        "        cfreq[condition][token] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcx6_55RqXAU"
      },
      "source": [
        "Print the frequency distributions for all conditions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z0dXrb3qXAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9118b2d9-dc7a-4943-e4f2-0af1d80c8bb9"
      },
      "source": [
        "cfreq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConditionalFreqDist(nltk.probability.FreqDist,\n",
              "                    {2015: FreqDist({'?': 1,\n",
              "                               'hello': 1,\n",
              "                               'today': 1,\n",
              "                               'world': 1}),\n",
              "                     2016: FreqDist({'also': 1,\n",
              "                               'nice': 2,\n",
              "                               'weather': 1,\n",
              "                               'world': 1}),\n",
              "                     2017: FreqDist({',': 1,\n",
              "                               'also': 1,\n",
              "                               u'bicycle': 1,\n",
              "                               'nice': 1,\n",
              "                               'two': 1,\n",
              "                               u'wa': 1,\n",
              "                               'weather': 1,\n",
              "                               'yesterday': 1}),\n",
              "                     2018: FreqDist({'bicycle': 1, 'love': 1, 'ride': 1})})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWHOlBB_qXAV"
      },
      "source": [
        "Print the frequency distributions of the year 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zejWM_dqXAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d051c1-0d52-445f-a23f-0fda30c5adde"
      },
      "source": [
        "cfreq[2017]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 1,\n",
              "          'also': 1,\n",
              "          u'bicycle': 1,\n",
              "          'nice': 1,\n",
              "          'two': 1,\n",
              "          u'wa': 1,\n",
              "          'weather': 1,\n",
              "          'yesterday': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34QLgxiTKlVN"
      },
      "source": [
        "# Time series of word occurences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi0AhH7mqXAV"
      },
      "source": [
        "For all years between 2010 and 2020, get the frequency of the word \"nice\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PKo19RFMqXAX"
      },
      "source": [
        "word = \"world\"\n",
        "years = range(2010,2020)\n",
        "occurences = []\n",
        "for year in years:\n",
        "    occurences.append(cfreq[year][word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0X4jlZ0qXAX"
      },
      "source": [
        "Print the resulting time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDcL0Ed5qXAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26623a6-96f0-4a37-b744-adca148021ce"
      },
      "source": [
        "occurences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZYCBOauK4oK"
      },
      "source": [
        "Merge the years and the word occurcences in one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "uG4vql4MK3rM",
        "outputId": "157fcc55-73dc-439f-f2ab-932b39d97dc5"
      },
      "source": [
        "timeseries = pd.DataFrame(list(zip(years, occurences)),\n",
        "              columns=['years','count'])\n",
        "timeseries['years'] = pd.to_datetime(timeseries['years'], format='%Y')\n",
        "timeseries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       years  count\n",
              "0 2010-01-01      0\n",
              "1 2011-01-01      0\n",
              "2 2012-01-01      0\n",
              "3 2013-01-01      0\n",
              "4 2014-01-01      0\n",
              "5 2015-01-01      1\n",
              "6 2016-01-01      1\n",
              "7 2017-01-01      0\n",
              "8 2018-01-01      0\n",
              "9 2019-01-01      0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>years</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015-01-01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGGmV3kqXAX"
      },
      "source": [
        "Plot the time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPP0bsYaqXAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "6471df50-3794-4b6a-cf13-aa2610a6b020"
      },
      "source": [
        "alt.Chart(timeseries).mark_line().encode(\n",
        "    x='years',\n",
        "    y='count'\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@5\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@3.4.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@5\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "    (function(vegaEmbed) {\n",
              "      var spec = {\"selection\": {\"selector005\": {\"bind\": \"scales\", \"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}}, \"encoding\": {\"y\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"years\", \"type\": \"temporal\"}}, \"config\": {\"view\": {\"width\": 400, \"height\": 300}, \"mark\": {\"tooltip\": null}}, \"mark\": \"line\", \"datasets\": {\"data-0e943ecc478840a186cc8080687bcc79\": [{\"count\": 0, \"years\": \"2010-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2011-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2012-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2013-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2014-01-01T00:00:00\"}, {\"count\": 1, \"years\": \"2015-01-01T00:00:00\"}, {\"count\": 1, \"years\": \"2016-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2017-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2018-01-01T00:00:00\"}, {\"count\": 0, \"years\": \"2019-01-01T00:00:00\"}]}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.4.0.json\", \"data\": {\"name\": \"data-0e943ecc478840a186cc8080687bcc79\"}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "    })(vegaEmbed);\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLaOJSWqXAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}