{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "1.02 - Conditional word counting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olivermueller/aml4ta-2021/blob/main/Session_01/1_02_Conditional_word_counting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqqfUmU7Rsk_"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCDEMc3I3-y"
      },
      "source": [
        "# Set up Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ety5X8FRzHDD"
      },
      "source": [
        "# Install packages\n",
        "!pip install pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTr0oOHpRuwL"
      },
      "source": [
        "# <font color=\"#003660\">Week 1: Basics of Natural Language Processing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21pvAXnqXAF"
      },
      "source": [
        "# <font color=\"#003660\">Notebook 2: Conditional Word Counting</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
        "        ... load text data from files and databases,<br> \n",
        "        ... conduct basic NLP preprocessing (e.g., tokenization, stopword removal, stemming, lemmatization),<br>\n",
        "        ... calculate corpus statistics (esp. word frequencies), and<br>\n",
        "        ... calculate and visualize corpus statistics over time.\n",
        "    </font>\n",
        "</div>\n",
        "</center>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtu_FbD9qXAJ"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "As always, we first need to load a number of required Python packages:\n",
        "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
        "- `NLTK` is a leading platform for building Python programs to work with human language data.\n",
        "- `altair` is a visualization library based on the grammar of graphics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BAE0ESOwqXAK"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZVcs4V9JOhJ"
      },
      "source": [
        "To work with the `NLTK` package, you also need to download some additional data (e.g., stopword lists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bljTpl4JPtx"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PhOPcrmqXAM"
      },
      "source": [
        "# Load documents\n",
        "This time, we want to analyze documents with regards to some metadata (i.e., year of publication). Each document is stored in a dictionary with two keys (`text` and `year`). The corpus is stored as a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aV5f4froqXAM"
      },
      "source": [
        "corpus = [\n",
        "    {\"text\":\"Hello World\", \"year\":2015},\n",
        "    {\"text\":\"How are you today?\", \"year\":2015},\n",
        "    {\"text\":\"The world is nice\", \"year\":2016},\n",
        "    {\"text\":\"The weather is also nice\", \"year\":2016},\n",
        "    {\"text\":\"Yesterday, the weather was also nice\", \"year\":2017},\n",
        "    {\"text\":\"I own two bicycles\", \"year\":2017},\n",
        "    {\"text\":\"I love to ride my bicycle\", \"year\":2018}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1cHok_qXAM"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPv6koXpqXAQ"
      },
      "source": [
        "# Preprocess documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MpKatMPqXAQ"
      },
      "source": [
        "We make a copy of the corpus dictionary, iterate over its entries, and add a `tokens` field with the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZISE6qkJqXAR"
      },
      "source": [
        "docs_tokenized = corpus.copy()\n",
        "for i, entry in enumerate(docs_tokenized):\n",
        "    entry[\"tokens\"] = nltk.word_tokenize(entry[\"text\"])\n",
        "docs_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef289TzMqXAR"
      },
      "source": [
        "And we iterate again over the corpus to transform all tokens to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJtJqo0qXAR"
      },
      "source": [
        "docs_tokenized_lower = docs_tokenized.copy()\n",
        "for i,entry in enumerate(docs_tokenized_lower):\n",
        "    tokens_lower = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        tokens_lower.append(token.lower())\n",
        "    entry[\"tokens\"] = tokens_lower\n",
        "docs_tokenized_lower"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTjfiR6qXAS"
      },
      "source": [
        "And lemmatize all tokens..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NidxTidPqXAS"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "docs_tokenized_lower_lemmatized = docs_tokenized_lower.copy()\n",
        "for i,entry in enumerate(docs_tokenized_lower_lemmatized):\n",
        "    tokens_lemmatized = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
        "    entry[\"tokens\"] = tokens_lemmatized\n",
        "docs_tokenized_lower_lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-CKFTvqXAT"
      },
      "source": [
        "Finally, we iterate one last time over the corpus to remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41c8Sw83qXAT"
      },
      "source": [
        "docs_tokenized_lower_lemmatized_cleaned = docs_tokenized_lower_lemmatized.copy()\n",
        "for i,entry in enumerate(docs_tokenized_lower_lemmatized_cleaned):\n",
        "    tokens_cleaned = []\n",
        "    for token in entry[\"tokens\"]:\n",
        "        if (token.isalpha() and token not in stopwords.words('english')):\n",
        "            tokens_cleaned.append(token)\n",
        "    entry[\"tokens\"] = tokens_cleaned\n",
        "docs_tokenized_lower_lemmatized_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXXYgWwBqXAT"
      },
      "source": [
        "# Conditional word counting\n",
        "We seperately count words for each condition, that is, for each year. Unfortunately, this time we have to do this \"by hand\" and iterate through all docs and tokens and increase the token count for the respective condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PPKYDzHDqXAU"
      },
      "source": [
        "cfreq = nltk.ConditionalFreqDist()\n",
        "\n",
        "for doc in docs_tokenized_lower_lemmatized_cleaned:\n",
        "    for token in doc[\"tokens\"]:\n",
        "        condition = doc[\"year\"]\n",
        "        cfreq[condition][token] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcx6_55RqXAU"
      },
      "source": [
        "Print the frequency distributions for all conditions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z0dXrb3qXAU"
      },
      "source": [
        "cfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWHOlBB_qXAV"
      },
      "source": [
        "Print the frequency distributions of the year 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zejWM_dqXAV"
      },
      "source": [
        "cfreq[2017]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34QLgxiTKlVN"
      },
      "source": [
        "# Time series of word occurences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi0AhH7mqXAV"
      },
      "source": [
        "For all years between 2015 and 2018, get the frequency of the word \"nice\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PKo19RFMqXAX"
      },
      "source": [
        "word = \"world\"\n",
        "years = range(2015,2019)\n",
        "occurences = []\n",
        "for year in years:\n",
        "    occurences.append(cfreq[year][word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0X4jlZ0qXAX"
      },
      "source": [
        "Print the resulting time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDcL0Ed5qXAX"
      },
      "source": [
        "occurences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZYCBOauK4oK"
      },
      "source": [
        "Merge the years and the word occurcences in one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG4vql4MK3rM"
      },
      "source": [
        "timeseries = pd.DataFrame(list(zip(years, occurences)),\n",
        "              columns=['years','count'])\n",
        "timeseries['years'] = pd.to_datetime(timeseries['years'], format='%Y')\n",
        "timeseries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGGmV3kqXAX"
      },
      "source": [
        "Plot the time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPP0bsYaqXAY"
      },
      "source": [
        "alt.Chart(timeseries).mark_line().encode(\n",
        "    x='years',\n",
        "    y='count'\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLaOJSWqXAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}