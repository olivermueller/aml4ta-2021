{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"3.01 - Document classification.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# Seminar Applied Text Mining\n","## Session 3: Classifying Documents\n","## Notebook 1: Bag-of-words model with 1-grams and Logit classifier"]},{"cell_type":"markdown","metadata":{"id":"PD3Pb-p4sqps"},"source":["## Importing packages"]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is the fundamental package for scientific computing with Python.\n","- `itertools` provides functions for creating iterators for efficient looping through data structures.\n","- `json` allows to read and write JSON files.\n","- `spacy` offers industrial-strength natural language processing\n","- `sklearn` is the de-facto standard machine learning package in Python"]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt","executionInfo":{"status":"ok","timestamp":1632401633388,"user_tz":-120,"elapsed":1804,"user":{"displayName":"Oliver Mueller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12717968064814035358"}}},"source":["import pandas as pd\n","import numpy as np\n","import itertools\n","import json\n","import spacy\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZd82t53sqpu"},"source":["## Load documents"]},{"cell_type":"markdown","metadata":{"id":"IsrCafxksqpv"},"source":["Load the corpus of 10,000 Airline Tweets from a JSON file and display the first tweet."]},{"cell_type":"code","metadata":{"id":"vSIGfKcXsqpv","executionInfo":{"status":"error","timestamp":1632401633613,"user_tz":-120,"elapsed":232,"user":{"displayName":"Oliver Mueller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12717968064814035358"}},"outputId":"bd0787be-9e6b-4d7f-9507-e7be885956a3","colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["docs = json.loads(open('/Users/oliver/Dropbox/10 - Lehre/UPB/Applied Text Mining/Code and Datasets/AirlineTweets.json').read())\n","docs[0]"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5caff6c3a557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/oliver/Dropbox/10 - Lehre/UPB/Applied Text Mining/Code and Datasets/AirlineTweets.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/oliver/Dropbox/10 - Lehre/UPB/Applied Text Mining/Code and Datasets/AirlineTweets.json'"]}]},{"cell_type":"markdown","metadata":{"id":"1v8oiPAcsqpx"},"source":["## Prepare documents"]},{"cell_type":"markdown","metadata":{"id":"bSrHVdoZsqpy"},"source":["Perform standard NLP preparation steps with spaCy."]},{"cell_type":"code","metadata":{"id":"sh4KVmP6sqpy"},"source":["nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n","\n","for i, entry in enumerate(docs):\n","    text = nlp(entry[u'text'])\n","    tokens_to_keep = []\n","    for token in text:\n","        if token.is_alpha and not token.is_stop: # see with what other tags spaCy has annotated the tokens: https://spacy.io/api/token#attributes1\n","            tokens_to_keep.append(token.lemma_)\n","    entry[u'text_prep'] = \" \".join(tokens_to_keep) # the .join turns the list into a concatenated string"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Tothp_Ssqpz"},"source":["<br>\n","Transform results into a data frame and display the first couple of lines."]},{"cell_type":"code","metadata":{"id":"UyLoiearsqpz","outputId":"1aba0768-de90-47e9-d44a-602589b57f72"},"source":["docs_df = pd.DataFrame(docs)\n","docs_df.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>airline</th>\n","      <th>date</th>\n","      <th>retweet_count</th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","      <th>text_prep</th>\n","      <th>tweet_created</th>\n","      <th>tweet_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>American</td>\n","      <td>2015-02-23 05:08:53 -0800</td>\n","      <td>0</td>\n","      <td>positive</td>\n","      <td>@AmericanAir thank you for doing the best you ...</td>\n","      <td>thank good rebook agent phone amp addtl resolu...</td>\n","      <td>2015-02-23</td>\n","      <td>5.698464e+17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>American</td>\n","      <td>2015-02-22 20:27:10 -0800</td>\n","      <td>0</td>\n","      <td>positive</td>\n","      <td>@AmericanAir wow that's helpful.</td>\n","      <td>wow helpful</td>\n","      <td>2015-02-22</td>\n","      <td>5.697151e+17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>United</td>\n","      <td>2015-02-17 14:32:23 -0800</td>\n","      <td>0</td>\n","      <td>negative</td>\n","      <td>@united so I wasted 40mins filling in 2 online...</td>\n","      <td>-PRON- waste fill online form tell receive -PR...</td>\n","      <td>2015-02-17</td>\n","      <td>5.678138e+17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>American</td>\n","      <td>2015-02-24 06:43:15 -0800</td>\n","      <td>0</td>\n","      <td>negative</td>\n","      <td>@AmericanAir my seat is disgusting. Old and di...</td>\n","      <td>seat disgusting old dirty when go refurbish pl...</td>\n","      <td>2015-02-24</td>\n","      <td>5.702325e+17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>US Airways</td>\n","      <td>2015-02-22 17:26:18 -0800</td>\n","      <td>0</td>\n","      <td>negative</td>\n","      <td>@USAirways ur specialist said they would talk ...</td>\n","      <td>ur specialist say talk stewardess serve drunk ...</td>\n","      <td>2015-02-22</td>\n","      <td>5.696695e+17</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      airline                       date  retweet_count sentiment  \\\n","0    American  2015-02-23 05:08:53 -0800              0  positive   \n","1    American  2015-02-22 20:27:10 -0800              0  positive   \n","2      United  2015-02-17 14:32:23 -0800              0  negative   \n","3    American  2015-02-24 06:43:15 -0800              0  negative   \n","4  US Airways  2015-02-22 17:26:18 -0800              0  negative   \n","\n","                                                text  \\\n","0  @AmericanAir thank you for doing the best you ...   \n","1                   @AmericanAir wow that's helpful.   \n","2  @united so I wasted 40mins filling in 2 online...   \n","3  @AmericanAir my seat is disgusting. Old and di...   \n","4  @USAirways ur specialist said they would talk ...   \n","\n","                                           text_prep tweet_created  \\\n","0  thank good rebook agent phone amp addtl resolu...    2015-02-23   \n","1                                        wow helpful    2015-02-22   \n","2  -PRON- waste fill online form tell receive -PR...    2015-02-17   \n","3  seat disgusting old dirty when go refurbish pl...    2015-02-24   \n","4  ur specialist say talk stewardess serve drunk ...    2015-02-22   \n","\n","       tweet_id  \n","0  5.698464e+17  \n","1  5.697151e+17  \n","2  5.678138e+17  \n","3  5.702325e+17  \n","4  5.696695e+17  "]},"execution_count":37,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"q-kQ-qLGsqp0"},"source":["<br>\n","Split corpus into training (80%) and test (20%) sets."]},{"cell_type":"code","metadata":{"id":"2ZK7_G9xsqp0","outputId":"fcd01395-5b1d-4ad3-f0fd-88e8ea57f801"},"source":["docs_df_train = docs_df.iloc[0:8000,]\n","print docs_df_train.shape\n","docs_df_test = docs_df.iloc[8000:10000,]\n","print docs_df_test.shape"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(8000, 8)\n","(2000, 8)\n"]}]},{"cell_type":"markdown","metadata":{"id":"uAXDnM1Hsqp1"},"source":["<br>\n","Initialize a CountVectorizer object to turn texts into term-document matrix with term frequency as cell values."]},{"cell_type":"code","metadata":{"id":"WbeTq9S9sqp1"},"source":["count_vect = CountVectorizer(min_df=2, ngram_range=[1,3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06DEGrExsqp1"},"source":["<br>\n","Apply the CountVectorizer object to the training set. Ignore terms that appear in less than 10 documents."]},{"cell_type":"code","metadata":{"id":"AemriJOAsqp1","outputId":"87e4244a-e1d8-416e-d9c5-6c2e89451185"},"source":["X = count_vect.fit_transform(docs_df_train[\"text_prep\"].tolist())\n","print X.shape"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(8000, 11467)\n"]}]},{"cell_type":"markdown","metadata":{"id":"1OV17A_2sqp2"},"source":["<br>\n","Display an extract of the term-document matrix"]},{"cell_type":"code","metadata":{"id":"ANQd_dVlsqp2","outputId":"903cb418-6355-4c76-cd63-4c81f20abfb5"},"source":["X[90:95,90:95].todense()"],"execution_count":null,"outputs":[{"data":{"text/plain":["matrix([[0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0]])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"skRgSRS-sqp2"},"source":["<br>\n","Turn frequency counts into tf-idf values."]},{"cell_type":"code","metadata":{"id":"nl0sGjddsqp3","outputId":"04dc547d-b792-4632-baf8-1019d8c78066"},"source":["tfidf_transformer = TfidfTransformer().fit(X)\n","X = tfidf_transformer.transform(X)\n","X[90:95,90:95].todense()"],"execution_count":null,"outputs":[{"data":{"text/plain":["matrix([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"7QTlJ4BDsqp3"},"source":["<br>\n","Extract the labels that we want to predict from the training set."]},{"cell_type":"code","metadata":{"id":"gUpaU5Pgsqp3","outputId":"7322a7a6-78b1-408f-bb5b-f8aa2820be1b"},"source":["Y = docs_df_train[\"sentiment\"]\n","Y.head()"],"execution_count":null,"outputs":[{"data":{"text/plain":["0    positive\n","1    positive\n","2    negative\n","3    negative\n","4    negative\n","Name: sentiment, dtype: object"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"biwWaNjNsqp4"},"source":["## Train classifier on training set"]},{"cell_type":"markdown","metadata":{"id":"WTHaUsAosqp4"},"source":["Perform a logistic regression classification with the term-document matrix as the input variables (or features, indepdendent variables) and the sentiment classes as the target variable (or label, or dependent variable)."]},{"cell_type":"code","metadata":{"id":"YZkAh7oesqp4"},"source":["clf = LogisticRegression().fit(X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuAVWgC8sqp4"},"source":["<br>\n","Test whether classifier is working by predicting the sentiment of some fake tweets. We are reusing the count_vect and tfidf_transformer objects to apply the same preprocessing steps to the new data (in a real use case, we would also preprocess the new documents with spaCy). "]},{"cell_type":"code","metadata":{"id":"_B16iOiWsqp5","outputId":"2349f773-ee43-4695-afb0-8f954df5656e"},"source":["docs_new = ['I love Delta', 'I love aa']\n","X_new = count_vect.transform(docs_new)\n","X_new = tfidf_transformer.transform(X_new)\n","predicted = clf.predict(X_new)\n","print predicted"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[u'positive' u'positive']\n"]}]},{"cell_type":"markdown","metadata":{"id":"n6zZzlq4sqp5"},"source":["<br>\n","Instead of predicting binary labels, we can also predict the probability of a `positive` or `negative` label."]},{"cell_type":"code","metadata":{"id":"Ks6yRPyEsqp5","outputId":"e680bb10-3d6b-4ef2-a02d-ba5a462acfde"},"source":["predicted_prob = clf.predict_proba(X_new)\n","print clf.classes_\n","print predicted_prob"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[u'negative' u'positive']\n","[[0.26238916 0.73761084]\n"," [0.18022177 0.81977823]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"663LU5XQsqp5"},"source":["## Evaluate accuracy on test set"]},{"cell_type":"markdown","metadata":{"id":"K8-z8rqVsqp6"},"source":["Instead of just testing on two fake tweets, we evaluate the predictive accurcay of our model on the test set. Again, we reuse the count_vect and tfidf_transformer objects."]},{"cell_type":"code","metadata":{"id":"eiUIhoK0sqp6"},"source":["X_test = count_vect.transform(docs_df_test[\"text_prep\"])\n","X_test = tfidf_transformer.transform(X_test)\n","Y_test = docs_df_test[\"sentiment\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4upAshmAsqp6"},"source":["<br>\n","Call the predict function of our model with the test data and calculate precision, recall and F1-score."]},{"cell_type":"code","metadata":{"id":"3Ngkprkgsqp6","outputId":"93198b8d-c3ec-4f8e-c68c-3d43b7caef66"},"source":["predicted = clf.predict(X_test)\n","print metrics.classification_report(Y_test, predicted)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["             precision    recall  f1-score   support\n","\n","   negative       0.87      0.99      0.93      1561\n","   positive       0.92      0.49      0.64       439\n","\n","avg / total       0.88      0.88      0.86      2000\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"sOCv-dHNsqp6"},"source":["## Look at model coefficients"]},{"cell_type":"markdown","metadata":{"id":"oB06C4vFsqp7"},"source":["Logistic regression is typically not the most accurate classification model, but one big advantage is that it can be interpreted by looking at the coefficients of the input features."]},{"cell_type":"code","metadata":{"id":"EOHsVlNGsqp7"},"source":["coeffs = clf.coef_[0].tolist()\n","words = count_vect.get_feature_names()\n","words_with_coeffs = pd.DataFrame(coeffs, words, columns=[\"coeff\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0k6vJ0Ssqp7"},"source":["Here are the coefficient of the input features."]},{"cell_type":"code","metadata":{"id":"E_szkYITsqp7","outputId":"b168533b-5cfd-47c1-c83f-15a0057fc63d"},"source":["words_with_coeffs.head(10)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>aa</th>\n","      <td>-0.291471</td>\n","    </tr>\n","    <tr>\n","      <th>aa agent</th>\n","      <td>-0.041178</td>\n","    </tr>\n","    <tr>\n","      <th>aa choice</th>\n","      <td>-0.104358</td>\n","    </tr>\n","    <tr>\n","      <th>aa choice bother</th>\n","      <td>-0.027953</td>\n","    </tr>\n","    <tr>\n","      <th>aa dallas</th>\n","      <td>0.250298</td>\n","    </tr>\n","    <tr>\n","      <th>aa dallas only</th>\n","      <td>0.250298</td>\n","    </tr>\n","    <tr>\n","      <th>aa email</th>\n","      <td>-0.024269</td>\n","    </tr>\n","    <tr>\n","      <th>aa employee</th>\n","      <td>-0.072930</td>\n","    </tr>\n","    <tr>\n","      <th>aa employee rude</th>\n","      <td>-0.038766</td>\n","    </tr>\n","    <tr>\n","      <th>aa flight</th>\n","      <td>0.224420</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     coeff\n","aa               -0.291471\n","aa agent         -0.041178\n","aa choice        -0.104358\n","aa choice bother -0.027953\n","aa dallas         0.250298\n","aa dallas only    0.250298\n","aa email         -0.024269\n","aa employee      -0.072930\n","aa employee rude -0.038766\n","aa flight         0.224420"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"gaHsYC1xsqp7"},"source":["<br>\n","These are the words with the most negative impact.    "]},{"cell_type":"code","metadata":{"id":"uc6X4kaFsqp7","outputId":"a7719659-8563-4e74-de1a-0e3f2a4983a1"},"source":["words_with_coeffs.sort_values(\"coeff\", ascending=True).head(100)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>hour</th>\n","      <td>-4.051477</td>\n","    </tr>\n","    <tr>\n","      <th>delay</th>\n","      <td>-3.549587</td>\n","    </tr>\n","    <tr>\n","      <th>bad</th>\n","      <td>-3.262633</td>\n","    </tr>\n","    <tr>\n","      <th>hold</th>\n","      <td>-3.024519</td>\n","    </tr>\n","    <tr>\n","      <th>bag</th>\n","      <td>-2.369616</td>\n","    </tr>\n","    <tr>\n","      <th>cancel</th>\n","      <td>-2.236620</td>\n","    </tr>\n","    <tr>\n","      <th>say</th>\n","      <td>-2.220612</td>\n","    </tr>\n","    <tr>\n","      <th>tell</th>\n","      <td>-2.216237</td>\n","    </tr>\n","    <tr>\n","      <th>luggage</th>\n","      <td>-2.184288</td>\n","    </tr>\n","    <tr>\n","      <th>sit</th>\n","      <td>-2.175515</td>\n","    </tr>\n","    <tr>\n","      <th>flight</th>\n","      <td>-2.059672</td>\n","    </tr>\n","    <tr>\n","      <th>pay</th>\n","      <td>-2.017012</td>\n","    </tr>\n","    <tr>\n","      <th>try</th>\n","      <td>-1.930746</td>\n","    </tr>\n","    <tr>\n","      <th>rude</th>\n","      <td>-1.799786</td>\n","    </tr>\n","    <tr>\n","      <th>not</th>\n","      <td>-1.777133</td>\n","    </tr>\n","    <tr>\n","      <th>miss</th>\n","      <td>-1.751696</td>\n","    </tr>\n","    <tr>\n","      <th>minute</th>\n","      <td>-1.696089</td>\n","    </tr>\n","    <tr>\n","      <th>need</th>\n","      <td>-1.642116</td>\n","    </tr>\n","    <tr>\n","      <th>fail</th>\n","      <td>-1.613002</td>\n","    </tr>\n","    <tr>\n","      <th>rebook</th>\n","      <td>-1.580239</td>\n","    </tr>\n","    <tr>\n","      <th>seat</th>\n","      <td>-1.571380</td>\n","    </tr>\n","    <tr>\n","      <th>lose</th>\n","      <td>-1.566184</td>\n","    </tr>\n","    <tr>\n","      <th>why</th>\n","      <td>-1.563053</td>\n","    </tr>\n","    <tr>\n","      <th>no</th>\n","      <td>-1.543894</td>\n","    </tr>\n","    <tr>\n","      <th>customer</th>\n","      <td>-1.484730</td>\n","    </tr>\n","    <tr>\n","      <th>passenger</th>\n","      <td>-1.476637</td>\n","    </tr>\n","    <tr>\n","      <th>system</th>\n","      <td>-1.473936</td>\n","    </tr>\n","    <tr>\n","      <th>reservation</th>\n","      <td>-1.439410</td>\n","    </tr>\n","    <tr>\n","      <th>website</th>\n","      <td>-1.362585</td>\n","    </tr>\n","    <tr>\n","      <th>happen</th>\n","      <td>-1.362484</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>waste</th>\n","      <td>-0.956648</td>\n","    </tr>\n","    <tr>\n","      <th>fuck</th>\n","      <td>-0.940003</td>\n","    </tr>\n","    <tr>\n","      <th>charge</th>\n","      <td>-0.933412</td>\n","    </tr>\n","    <tr>\n","      <th>different</th>\n","      <td>-0.930353</td>\n","    </tr>\n","    <tr>\n","      <th>error</th>\n","      <td>-0.919717</td>\n","    </tr>\n","    <tr>\n","      <th>pron need</th>\n","      <td>-0.911443</td>\n","    </tr>\n","    <tr>\n","      <th>cancel flightl</th>\n","      <td>-0.902198</td>\n","    </tr>\n","    <tr>\n","      <th>spend</th>\n","      <td>-0.889645</td>\n","    </tr>\n","    <tr>\n","      <th>close</th>\n","      <td>-0.879359</td>\n","    </tr>\n","    <tr>\n","      <th>flight cancel</th>\n","      <td>-0.879299</td>\n","    </tr>\n","    <tr>\n","      <th>there</th>\n","      <td>-0.878929</td>\n","    </tr>\n","    <tr>\n","      <th>lie</th>\n","      <td>-0.858261</td>\n","    </tr>\n","    <tr>\n","      <th>feedback</th>\n","      <td>-0.857231</td>\n","    </tr>\n","    <tr>\n","      <th>travel</th>\n","      <td>-0.855479</td>\n","    </tr>\n","    <tr>\n","      <th>shit</th>\n","      <td>-0.850067</td>\n","    </tr>\n","    <tr>\n","      <th>change</th>\n","      <td>-0.835330</td>\n","    </tr>\n","    <tr>\n","      <th>anymore</th>\n","      <td>-0.832833</td>\n","    </tr>\n","    <tr>\n","      <th>half</th>\n","      <td>-0.828932</td>\n","    </tr>\n","    <tr>\n","      <th>flightled</th>\n","      <td>-0.822504</td>\n","    </tr>\n","    <tr>\n","      <th>suppose</th>\n","      <td>-0.819508</td>\n","    </tr>\n","    <tr>\n","      <th>pron pay</th>\n","      <td>-0.818860</td>\n","    </tr>\n","    <tr>\n","      <th>policy</th>\n","      <td>-0.810170</td>\n","    </tr>\n","    <tr>\n","      <th>want</th>\n","      <td>-0.801139</td>\n","    </tr>\n","    <tr>\n","      <th>list</th>\n","      <td>-0.797123</td>\n","    </tr>\n","    <tr>\n","      <th>agent</th>\n","      <td>-0.784891</td>\n","    </tr>\n","    <tr>\n","      <th>app</th>\n","      <td>-0.783788</td>\n","    </tr>\n","    <tr>\n","      <th>cost</th>\n","      <td>-0.779044</td>\n","    </tr>\n","    <tr>\n","      <th>horrible</th>\n","      <td>-0.778374</td>\n","    </tr>\n","    <tr>\n","      <th>expect</th>\n","      <td>-0.776499</td>\n","    </tr>\n","    <tr>\n","      <th>cancel flightled</th>\n","      <td>-0.776469</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows Ã— 1 columns</p>\n","</div>"],"text/plain":["                     coeff\n","hour             -4.051477\n","delay            -3.549587\n","bad              -3.262633\n","hold             -3.024519\n","bag              -2.369616\n","cancel           -2.236620\n","say              -2.220612\n","tell             -2.216237\n","luggage          -2.184288\n","sit              -2.175515\n","flight           -2.059672\n","pay              -2.017012\n","try              -1.930746\n","rude             -1.799786\n","not              -1.777133\n","miss             -1.751696\n","minute           -1.696089\n","need             -1.642116\n","fail             -1.613002\n","rebook           -1.580239\n","seat             -1.571380\n","lose             -1.566184\n","why              -1.563053\n","no               -1.543894\n","customer         -1.484730\n","passenger        -1.476637\n","system           -1.473936\n","reservation      -1.439410\n","website          -1.362585\n","happen           -1.362484\n","...                    ...\n","waste            -0.956648\n","fuck             -0.940003\n","charge           -0.933412\n","different        -0.930353\n","error            -0.919717\n","pron need        -0.911443\n","cancel flightl   -0.902198\n","spend            -0.889645\n","close            -0.879359\n","flight cancel    -0.879299\n","there            -0.878929\n","lie              -0.858261\n","feedback         -0.857231\n","travel           -0.855479\n","shit             -0.850067\n","change           -0.835330\n","anymore          -0.832833\n","half             -0.828932\n","flightled        -0.822504\n","suppose          -0.819508\n","pron pay         -0.818860\n","policy           -0.810170\n","want             -0.801139\n","list             -0.797123\n","agent            -0.784891\n","app              -0.783788\n","cost             -0.779044\n","horrible         -0.778374\n","expect           -0.776499\n","cancel flightled -0.776469\n","\n","[100 rows x 1 columns]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"xo2Zz0qrsqp8"},"source":["<br>\n","And these are the words with the most positive impact."]},{"cell_type":"code","metadata":{"id":"FBYjvIJnsqp8","outputId":"e3d75c9e-bd6b-4eef-c304-9f300d0a1b4b"},"source":["words_with_coeffs.sort_values(\"coeff\", ascending=False).head(100)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>thank</th>\n","      <td>11.050532</td>\n","    </tr>\n","    <tr>\n","      <th>great</th>\n","      <td>5.644050</td>\n","    </tr>\n","    <tr>\n","      <th>awesome</th>\n","      <td>4.682721</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>4.639319</td>\n","    </tr>\n","    <tr>\n","      <th>good</th>\n","      <td>3.870915</td>\n","    </tr>\n","    <tr>\n","      <th>amazing</th>\n","      <td>3.624084</td>\n","    </tr>\n","    <tr>\n","      <th>thx</th>\n","      <td>2.999760</td>\n","    </tr>\n","    <tr>\n","      <th>appreciate</th>\n","      <td>2.889685</td>\n","    </tr>\n","    <tr>\n","      <th>excellent</th>\n","      <td>2.253851</td>\n","    </tr>\n","    <tr>\n","      <th>wonderful</th>\n","      <td>2.221593</td>\n","    </tr>\n","    <tr>\n","      <th>rock</th>\n","      <td>2.079320</td>\n","    </tr>\n","    <tr>\n","      <th>cool</th>\n","      <td>2.002845</td>\n","    </tr>\n","    <tr>\n","      <th>happy</th>\n","      <td>1.900865</td>\n","    </tr>\n","    <tr>\n","      <th>thank pron</th>\n","      <td>1.845831</td>\n","    </tr>\n","    <tr>\n","      <th>haha</th>\n","      <td>1.818617</td>\n","    </tr>\n","    <tr>\n","      <th>crew</th>\n","      <td>1.787828</td>\n","    </tr>\n","    <tr>\n","      <th>pron love</th>\n","      <td>1.736510</td>\n","    </tr>\n","    <tr>\n","      <th>kudo</th>\n","      <td>1.637033</td>\n","    </tr>\n","    <tr>\n","      <th>jetblue</th>\n","      <td>1.607790</td>\n","    </tr>\n","    <tr>\n","      <th>thank help</th>\n","      <td>1.591260</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                coeff\n","thank       11.050532\n","great        5.644050\n","awesome      4.682721\n","love         4.639319\n","good         3.870915\n","amazing      3.624084\n","thx          2.999760\n","appreciate   2.889685\n","excellent    2.253851\n","wonderful    2.221593\n","rock         2.079320\n","cool         2.002845\n","happy        1.900865\n","thank pron   1.845831\n","haha         1.818617\n","crew         1.787828\n","pron love    1.736510\n","kudo         1.637033\n","jetblue      1.607790\n","thank help   1.591260"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"AyxBff2Ysqp8"},"source":[""],"execution_count":null,"outputs":[]}]}