{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Applied Text Mining\n",
    "## Session 3: Classifying Documents\n",
    "## Notebook 2: Tune classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we first need to load a number of required Python packages:\n",
    "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "- `numpy` is the fundamental package for scientific computing with Python.\n",
    "- `itertools` provides functions for creating iterators for efficient looping through data structures.\n",
    "- `json` allows to read and write JSON files.\n",
    "- `spacy` offers industrial-strength natural language processing\n",
    "- `sklearn` is the de-facto standard machine learning package in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corpus of 10,000 Airline Tweets from a JSON file and display the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'airline': u'American',\n",
       " u'date': u'2015-02-23 05:08:53 -0800',\n",
       " u'retweet_count': 0,\n",
       " u'sentiment': u'positive',\n",
       " u'text': u'@AmericanAir thank you for doing the best you could to get me rebooked. Agent on phone &amp; addtl resolution on DM was very much appreciated.',\n",
       " u'tweet_created': u'2015-02-23',\n",
       " u'tweet_id': 5.6984635640934e+17}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = json.loads(open('/Users/oliver/Dropbox/10 - Lehre/UPB/Applied Text Mining/Code and Datasets/AirlineTweets.json').read())\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform standard NLP preparation steps with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "for i, entry in enumerate(docs):\n",
    "    text = nlp(entry[u'text'])\n",
    "    tokens_to_keep = []\n",
    "    for token in text:\n",
    "        if token.is_alpha and not token.is_stop: # see with what other tags spaCy has annotated the tokens: https://spacy.io/api/token#attributes1\n",
    "            tokens_to_keep.append(token.lemma_)\n",
    "    entry[u'text_prep'] = \" \".join(tokens_to_keep) # the .join turns the list into a concatenated string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Transform results into a data frame and display the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_prep</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American</td>\n",
       "      <td>2015-02-23 05:08:53 -0800</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you for doing the best you ...</td>\n",
       "      <td>thank good rebook agent phone amp addtl resolu...</td>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>5.698464e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American</td>\n",
       "      <td>2015-02-22 20:27:10 -0800</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir wow that's helpful.</td>\n",
       "      <td>wow helpful</td>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>5.697151e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "      <td>2015-02-17 14:32:23 -0800</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>@united so I wasted 40mins filling in 2 online...</td>\n",
       "      <td>-PRON- waste fill online form tell receive -PR...</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>5.678138e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American</td>\n",
       "      <td>2015-02-24 06:43:15 -0800</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir my seat is disgusting. Old and di...</td>\n",
       "      <td>seat disgusting old dirty when go refurbish pl...</td>\n",
       "      <td>2015-02-24</td>\n",
       "      <td>5.702325e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US Airways</td>\n",
       "      <td>2015-02-22 17:26:18 -0800</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>@USAirways ur specialist said they would talk ...</td>\n",
       "      <td>ur specialist say talk stewardess serve drunk ...</td>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>5.696695e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline                       date  retweet_count sentiment  \\\n",
       "0    American  2015-02-23 05:08:53 -0800              0  positive   \n",
       "1    American  2015-02-22 20:27:10 -0800              0  positive   \n",
       "2      United  2015-02-17 14:32:23 -0800              0  negative   \n",
       "3    American  2015-02-24 06:43:15 -0800              0  negative   \n",
       "4  US Airways  2015-02-22 17:26:18 -0800              0  negative   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AmericanAir thank you for doing the best you ...   \n",
       "1                   @AmericanAir wow that's helpful.   \n",
       "2  @united so I wasted 40mins filling in 2 online...   \n",
       "3  @AmericanAir my seat is disgusting. Old and di...   \n",
       "4  @USAirways ur specialist said they would talk ...   \n",
       "\n",
       "                                           text_prep tweet_created  \\\n",
       "0  thank good rebook agent phone amp addtl resolu...    2015-02-23   \n",
       "1                                        wow helpful    2015-02-22   \n",
       "2  -PRON- waste fill online form tell receive -PR...    2015-02-17   \n",
       "3  seat disgusting old dirty when go refurbish pl...    2015-02-24   \n",
       "4  ur specialist say talk stewardess serve drunk ...    2015-02-22   \n",
       "\n",
       "       tweet_id  \n",
       "0  5.698464e+17  \n",
       "1  5.697151e+17  \n",
       "2  5.678138e+17  \n",
       "3  5.702325e+17  \n",
       "4  5.696695e+17  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df = pd.DataFrame(docs)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Split corpus into training (80%) and test (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 8)\n",
      "(2000, 8)\n"
     ]
    }
   ],
   "source": [
    "docs_df_train = docs_df.iloc[0:8000,]\n",
    "print docs_df_train.shape\n",
    "docs_df_test = docs_df.iloc[8000:10000,]\n",
    "print docs_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate multiple classifiers with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the vectorizer => transformer => classifier process easier to work with, scikit-learn provides the Pipeline class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_01 = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df = 2)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can now preprocess the texts and train a classifier with a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_01.fit(docs_df_train[\"text_prep\"], docs_df_train[\"sentiment\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "And then classify all documents in the test set and evaluate the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.88      0.99      0.93      1561\n",
      "   positive       0.91      0.51      0.66       439\n",
      "\n",
      "avg / total       0.89      0.88      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test = docs_df_test[\"sentiment\"]\n",
    "predicted = text_clf_01.predict(docs_df_test[\"text_prep\"])\n",
    "print metrics.classification_report(Y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=[1, 3], preprocessor=None, stop_words=None,\n",
       "        st...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_02 = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df = 2, ngram_range=[1,3])),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=500))\n",
    "])\n",
    "text_clf_02.fit(docs_df_train[\"text_prep\"], docs_df_train[\"sentiment\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.90      0.97      0.93      1561\n",
      "   positive       0.85      0.60      0.70       439\n",
      "\n",
      "avg / total       0.88      0.89      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test = docs_df_test[\"sentiment\"]\n",
    "predicted = text_clf_02.predict(docs_df_test[\"text_prep\"])\n",
    "print metrics.classification_report(Y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=[1, 3], preprocessor=None, stop_words=None,\n",
       "        st...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_03 = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df = 2, ngram_range=[1,3])),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "text_clf_03.fit(docs_df_train[\"text_prep\"], docs_df_train[\"sentiment\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.91      0.97      0.94      1561\n",
      "   positive       0.86      0.66      0.74       439\n",
      "\n",
      "avg / total       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test = docs_df_test[\"sentiment\"]\n",
    "predicted = text_clf_03.predict(docs_df_test[\"text_prep\"])\n",
    "print metrics.classification_report(Y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
